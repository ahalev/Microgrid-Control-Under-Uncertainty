env:
  cls: DiscreteMicrogridEnv
  observation_keys: []
  domain_randomization:
    noise_std: 0.0
    relative_noise: true
  forced_genset: null
  net_load:
    use: true
    slack_module: grid
microgrid:
  config:
    scenario: 0
  trajectory:
    train:
      initial_step: 0
      final_step: -1
      trajectory_func: null
    evaluate:
      initial_step: 0
      final_step: -1
      trajectory_func: null
  methods:
    set_forecaster:
      forecast_horizon: 23
      forecaster: 0.0
      forecaster_increase_uncertainty: false
      forecaster_relative_noise: false
    set_module_attrs:
      normalized_action_bounds: [-1.0, 1.0]
      battery_transition_model: null
  attributes:
    reward_shaping_func: null
algo:
  type: rl
  package: garage
  sampler:
    type: ray
    n_workers: 16
  policy:
    pretrained_policy: null
    hidden_sizes:
      - 32
      - 32
  replay_buffer:
    buffer_size: 200000
  general_params:
    discount: 0.99
  deterministic_params:
    buffer_batch_size: 32
    qf_lr: 0.0001
    min_buffer_size: 10000
    n_train_steps: 500
    steps_per_epoch: 1
  dqn:
    params:
      clip_gradient: 10
      deterministic_eval: true
      double_q: false
      target_update_freq: 2
    policy:
      exploration:
        decay_ratio: 0.5
        max_epsilon: 1.0
        min_epsilon: 0.05
  ddpg:
    params:
      target_update_tau: 0.01
    policy:
        exploration:
          sigma: 0.3
          theta: 0.15
  ppo:
    tanhnormal: false
    params:
      center_adv: false
  pretrain:
    algo_to_pretrain: null
    pretrain_algo: rbc
    params:
      loss: log_prob
      policy_lr: 0.01
      episodes_per_batch: 10
    additional_config: null
  rnd:
    intrinsic_reward_weight: 0.0
    bound_reward:
      weight: null
      transient_epochs: 10
      initial_ratio: 0.999999  # 1-1e-6
      ratio_of_means: true
      relative_to_ratio: true
    predictor_lr: 1.0e-3
    batch_size: 64
    hidden_sizes:
      - 64
      - 64
    output_dim: 128
    n_train_steps: 32
    standardize_extrinsic_reward: true
    standardize_intrinsic_reward: true
  train:
    batch_size: 50000
    n_epochs: 100
context:
  verbose: 0
  experiment_name: null
  disable_logging: false
  log_dir:
    parent: experiment_logs
    from_keys: []
    use_existing_dir: false
  snapshot_gap: 10
  seed: 1
  wandb:
    api_key_file: local/wandb_api_key.txt
    username: ahalev
    group: null
    log_density: 1
verbose: 0
