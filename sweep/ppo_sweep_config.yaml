program: ../trainer.py
project: gridrl
method: bayes
parameters:
  algo:
    type:
      value: ppo
    policy:
      hidden_sizes:
        values:
          - - 64
            - 64
          - - 128
            - 128
    ppo:
      tanhnormal:
        values: [true, false]
      params:
        center_adv:
          value: false
    rnd:
      intrinsic_reward_weight:
        distribution: log_uniform_values
        min: 1.0e-6
        max: 1.0
      predictor_lr:
        distribution: log_uniform_values
        min: 1.0e-6
        max: 1.0e-1
      hidden_sizes:
        value: [64, 64]
  env:
    domain_randomization:
      noise_std:
        distribution: uniform
        min: 0.0
        max: 1.0
    observation_keys:
      values:
        - ['soc', 'net_load']
        - ['soc', 'net_load', 'import_price_current']
        - ['soc', 'net_load', 'import_price_current', 'import_price_forecast_0', 'import_price_forecast_1']
        - ['soc', 'net_load', 'import_price_current', 'import_price_forecast_0', 'import_price_forecast_1', 'import_price_forecast_2', 'import_price_forecast_3']
        - ['soc', 'net_load', 'import_price_current', 'import_price_forecast_0', 'import_price_forecast_1', 'import_price_forecast_2', 'import_price_forecast_3', 'import_price_forecast_4', 'import_price_forecast_5']
    attributes:
      reward_shaping_func:
        values:
          - '!BaselineShaper {module: [grid,0]}'
          - '!SequentialShaper {shapers: [!BaselineShaper {module:[grid,0]}, !LearnedScaleRescaleShaper {} ]}'
metric:
  name: Test/MPCRelativeRewardSum
  goal: minimize
  target: 0.8
command:
  - ${envvar:CONDA_PREFIX}/bin/python
  - ${program}
  - "--config"
  - "general_config.yaml"
  - ${args}
meta:
  wandb_setup:
    api_key_file: ../local/wandb_api_key.txt
    username: ahalev
  sweep_id: null
  launch_agent: false
  scenario: 0
